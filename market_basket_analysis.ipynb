{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõí An√°lisis de Canasta de Mercado: Retail en Espa√±a\n",
    "\n",
    "Mi primer proyecto de an√°lisis de datos con Python.\n",
    "\n",
    "## Objetivos:\n",
    "1. Implementar el algoritmo Apriori desde cero\n",
    "2. Descubrir patrones de compra frecuentes  \n",
    "3. Visualizar las reglas de asociaci√≥n m√°s relevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generaci√≥n del Dataset\n",
    "\n",
    "Creamos transacciones realistas basadas en categor√≠as de productos espa√±oles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_dataset_transacciones(n_transacciones=8000, random_state=42):\n",
    "    \"\"\"Genera dataset de transacciones de supermercado espa√±ol.\"\"\"\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    categorias = {\n",
    "        'Alimentos_bebidas': ['Leche entera', 'Leche desnatada', 'Yogur natural', \n",
    "                              'Queso fresco', 'Mantequilla', 'Pan de barra', \n",
    "                              'Pan integral', 'Huevos', 'Aceite oliva'],\n",
    "        'Carne_pescado': ['Pollo entero', 'Filete ternera', 'Jam√≥n ib√©rico', \n",
    "                         'Chorizo', 'Pechuga pavo', 'Merluza', 'Salm√≥n', \n",
    "                         'Gambas', 'At√∫n enlatado'],\n",
    "        'Frutas_verduras': ['Manzanas', 'Pl√°tanos', 'Naranjas', 'Tomates', \n",
    "                           'Lechuga', 'Cebolla', 'Patatas', 'Zanahorias', 'Pimientos'],\n",
    "        'Bebidas': ['Agua mineral', 'Refresco cola', 'Zumo naranja', \n",
    "                   'Cerveza', 'Vino tinto', 'Vino blanco', 'Caf√© molido'],\n",
    "        'Despensa': ['Arroz', 'Pasta', 'Az√∫car', 'Sal', 'Conserva tomate', \n",
    "                    'Legumbres', 'Harina', 'Cereales'],\n",
    "        'Congelados': ['Pizza congelada', 'Helado vainilla', 'Verduras congeladas', \n",
    "                      'Pescado congelado', 'Croquetas'],\n",
    "        'Higiene_hogar': ['Papel higi√©nico', 'Champ√∫', 'Gel de ducha', \n",
    "                         'Detergente', 'Limpiador multiusos', 'Suavizante'],\n",
    "        'Dulces_snacks': ['Galletas chocolate', 'Chocolate negro', 'Turron', \n",
    "                         'Miel', 'Patatas fritas', 'Frutos secos']\n",
    "    }\n",
    "    \n",
    "    reglas = {\n",
    "        'Vino tinto': {'asociados': ['Queso fresco', 'Jam√≥n ib√©rico', 'Pan de barra'], 'prob': 0.65},\n",
    "        'Cerveza': {'asociados': ['Jam√≥n ib√©rico', 'Gambas', 'Patatas fritas'], 'prob': 0.60},\n",
    "        'Pan de barra': {'asociados': ['Mantequilla', 'Jam√≥n ib√©rico', 'Tomates'], 'prob': 0.55},\n",
    "        'Pasta': {'asociados': ['Tomates', 'Queso fresco', 'Aceite oliva'], 'prob': 0.70},\n",
    "        'Arroz': {'asociados': ['Pollo entero', 'Tomates', 'Aceite oliva'], 'prob': 0.60},\n",
    "        'Caf√© molido': {'asociados': ['Leche entera', 'Az√∫car', 'Galletas chocolate'], 'prob': 0.50},\n",
    "        'Gambas': {'asociados': ['Vino blanco', 'Pan de barra'], 'prob': 0.55},\n",
    "        'Pizza congelada': {'asociados': ['Refresco cola', 'Helado vainilla'], 'prob': 0.45}\n",
    "    }\n",
    "    \n",
    "    transacciones = []\n",
    "    \n",
    "    for _ in range(n_transacciones):\n",
    "        n_categorias = np.random.randint(2, 6)\n",
    "        cats_seleccionadas = np.random.choice(list(categorias.keys()), n_categorias, replace=False)\n",
    "        \n",
    "        carrito = []\n",
    "        for cat in cats_seleccionadas:\n",
    "            n_productos = np.random.randint(1, 4)\n",
    "            productos = np.random.choice(categorias[cat], min(n_productos, len(categorias[cat])), replace=False)\n",
    "            carrito.extend(productos)\n",
    "        \n",
    "        for producto_base, info in reglas.items():\n",
    "            if producto_base in carrito and np.random.random() < info['prob']:\n",
    "                asociados = np.random.choice(info['asociados'], np.random.randint(1, 3), replace=False)\n",
    "                carrito.extend(asociados)\n",
    "        \n",
    "        transacciones.append(list(set(carrito)))\n",
    "    \n",
    "    return transacciones, categorias\n",
    "\n",
    "print(\"üîÑ Generando dataset...\")\n",
    "transacciones, categorias = generar_dataset_transacciones()\n",
    "\n",
    "print(f\"‚úÖ Dataset generado:\")\n",
    "print(f\"   - Transacciones: {len(transacciones):,}\")\n",
    "print(f\"   - Productos √∫nicos: {len(set([item for sublist in transacciones for item in sublist]))}\")\n",
    "print(f\"   - Media productos/transacci√≥n: {np.mean([len(t) for t in transacciones]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementaci√≥n del Algoritmo Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AprioriAnalyzer:\n",
    "    \"\"\"Implementaci√≥n del algoritmo Apriori para an√°lisis de canasta de mercado.\"\"\"\n",
    "    \n",
    "    def __init__(self, transactions, min_support=0.03, min_confidence=0.40):\n",
    "        self.transactions = transactions\n",
    "        self.n_transactions = len(transactions)\n",
    "        self.min_support = min_support\n",
    "        self.min_confidence = min_confidence\n",
    "        self.frequent_itemsets = {}\n",
    "        self.rules = []\n",
    "        \n",
    "    def _get_itemsets(self, k):\n",
    "        \"\"\"Obtiene todos los itemsets de tama√±o k.\"\"\"\n",
    "        itemsets = Counter()\n",
    "        for transaction in self.transactions:\n",
    "            if len(transaction) >= k:\n",
    "                for combo in combinations(transaction, k):\n",
    "                    itemsets[frozenset(combo)] += 1\n",
    "        return itemsets\n",
    "    \n",
    "    def _filter_by_support(self, itemsets):\n",
    "        \"\"\"Filtra itemsets por soporte m√≠nimo.\"\"\"\n",
    "        return {itemset: count for itemset, count in itemsets.items() \n",
    "                if count / self.n_transactions >= self.min_support}\n",
    "    \n",
    "    def find_frequent_itemsets(self, max_k=2):\n",
    "        \"\"\"Encuentra itemsets frecuentes hasta tama√±o max_k.\"\"\"\n",
    "        print(f\"üîç Buscando itemsets frecuentes (support >= {self.min_support*100}%)...\")\n",
    "        \n",
    "        for k in range(1, max_k + 1):\n",
    "            itemsets = self._get_itemsets(k)\n",
    "            frequent = self._filter_by_support(itemsets)\n",
    "            \n",
    "            if not frequent:\n",
    "                break\n",
    "                \n",
    "            self.frequent_itemsets[k] = frequent\n",
    "            print(f\"   k={k}: {len(frequent)} itemsets encontrados\")\n",
    "        \n",
    "        return self.frequent_itemsets\n",
    "    \n",
    "    def generate_rules(self):\n",
    "        \"\"\"Genera reglas de asociaci√≥n a partir de itemsets frecuentes.\"\"\"\n",
    "        if 2 not in self.frequent_itemsets:\n",
    "            raise ValueError(\"No hay itemsets de tama√±o 2. Reduce min_support.\")\n",
    "        \n",
    "        print(f\"üìà Generando reglas (confidence >= {self.min_confidence*100}%)...\")\n",
    "        \n",
    "        frequent_1 = self.frequent_itemsets[1]\n",
    "        frequent_2 = self.frequent_itemsets[2]\n",
    "        \n",
    "        rules = []\n",
    "        \n",
    "        for itemset_2, count_2 in frequent_2.items():\n",
    "            items = list(itemset_2)\n",
    "            item_a, item_b = items[0], items[1]\n",
    "            \n",
    "            support = count_2 / self.n_transactions\n",
    "            \n",
    "            # Regla: A -> B\n",
    "            count_a = frequent_1.get(frozenset([item_a]), 0)\n",
    "            confidence_a_b = count_2 / count_a if count_a > 0 else 0\n",
    "            count_b = frequent_1.get(frozenset([item_b]), 0)\n",
    "            support_b = count_b / self.n_transactions\n",
    "            lift_a_b = confidence_a_b / support_b if support_b > 0 else 0\n",
    "            \n",
    "            if confidence_a_b >= self.min_confidence:\n",
    "                rules.append({\n",
    "                    'antecedent': item_a,\n",
    "                    'consequent': item_b,\n",
    "                    'support': support,\n",
    "                    'confidence': confidence_a_b,\n",
    "                    'lift': lift_a_b,\n",
    "                    'count': count_2\n",
    "                })\n",
    "            \n",
    "            # Regla: B -> A\n",
    "            confidence_b_a = count_2 / count_b if count_b > 0 else 0\n",
    "            support_a = count_a / self.n_transactions\n",
    "            lift_b_a = confidence_b_a / support_a if support_a > 0 else 0\n",
    "            \n",
    "            if confidence_b_a >= self.min_confidence:\n",
    "                rules.append({\n",
    "                    'antecedent': item_b,\n",
    "                    'consequent': item_a,\n",
    "                    'support': support,\n",
    "                    'confidence': confidence_b_a,\n",
    "                    'lift': lift_b_a,\n",
    "                    'count': count_2\n",
    "                })\n",
    "        \n",
    "        self.rules_df = pd.DataFrame(rules)\n",
    "        if not self.rules_df.empty:\n",
    "            self.rules_df = self.rules_df.sort_values('lift', ascending=False)\n",
    "        \n",
    "        print(f\"‚úÖ {len(self.rules_df)} reglas generadas\")\n",
    "        return self.rules_df\n",
    "\n",
    "# Ejecutar an√°lisis\n",
    "analyzer = AprioriAnalyzer(transacciones, min_support=0.03, min_confidence=0.40)\nfrequent_itemsets = analyzer.find_frequent_itemsets(max_k=2)\nrules_df = analyzer.generate_rules()\n\nprint(\"\\nüèÜ TOP 5 REGLAS POR LIFT:\")\nprint(\"-\" * 60)\nfor idx, rule in rules_df.head(5).iterrows():\n",
    "    print(f\"{rule['antecedent']} ‚Üí {rule['consequent']}\")\n",
    "    print(f\"   Soporte: {rule['support']:.1%} | Confianza: {rule['confidence']:.1%} | Lift: {rule['lift']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_market_basket_analysis(rules_df, transactions, categorias, output_path='images/'):\n",
    "    \"\"\"Genera visualizaciones del an√°lisis.\"\"\"\n",
    "    \n",
    "    import os\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    fig.suptitle('An√°lisis de Canasta de Mercado: Retail en Espa√±a', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # 1. Top reglas por Lift\n",
    "    ax1 = axes[0, 0]\n",
    "    top_rules = rules_df.head(10)\n",
    "    y_pos = np.arange(len(top_rules))\n",
    "    colors = plt.cm.RdYlGn(top_rules['lift'] / top_rules['lift'].max())\n",
    "    \n",
    "    bars = ax1.barh(y_pos, top_rules['lift'], color=colors)\n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels([f\"{row['antecedent'][:15]}...‚Üí{row['consequent'][:15]}...\" \n",
    "                         for _, row in top_rules.iterrows()], fontsize=9)\n",
    "    ax1.set_xlabel('Lift', fontweight='bold')\n",
    "    ax1.set_title('Top 10 Reglas por Lift', fontweight='bold', pad=20)\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 2. Scatter: Soporte vs Confianza\n",
    "    ax2 = axes[0, 1]\n",
    "    scatter = ax2.scatter(rules_df['support'] * 100, rules_df['confidence'] * 100, \n",
    "                         s=rules_df['lift'] * 50, c=rules_df['lift'], \n",
    "                         cmap='viridis', alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    ax2.set_xlabel('Soporte (%)', fontweight='bold')\n",
    "    ax2.set_ylabel('Confianza (%)', fontweight='bold')\n",
    "    ax2.set_title('Reglas: Soporte vs Confianza\\n(Tama√±o = Lift)', fontweight='bold', pad=20)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax2, label='Lift')\n",
    "    \n",
    "    # 3. Distribuci√≥n por categor√≠as\n",
    "    ax3 = axes[1, 0]\n",
    "    categoria_count = {}\n",
    "    for cat, productos in categorias.items():\n",
    "        count = sum(1 for t in transactions for p in productos if p in t)\n",
    "        categoria_count[cat] = count\n",
    "    \n",
    "    cat_df = pd.DataFrame(list(categoria_count.items()), columns=['Categor√≠a', 'Frecuencia'])\n",
    "    cat_df = cat_df.sort_values('Frecuencia', ascending=True)\n",
    "    \n",
    "    colors_cat = plt.cm.Set3(np.linspace(0, 1, len(cat_df)))\n",
    "    ax3.barh(cat_df['Categor√≠a'], cat_df['Frecuencia'], color=colors_cat)\n",
    "    ax3.set_xlabel('Frecuencia en Transacciones', fontweight='bold')\n",
    "    ax3.set_title('Distribuci√≥n por Categor√≠as', fontweight='bold', pad=20)\n",
    "    ax3.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 4. Matriz de co-ocurrencia\n",
    "    ax4 = axes[1, 1]\n",
    "    top_products = rules_df.head(8)\n",
    "    products_list = list(set(top_products['antecedent'].tolist() + \n",
    "                            top_products['consequent'].tolist()))\n",
    "    \n",
    "    cooccur_matrix = pd.DataFrame(0, index=products_list, columns=products_list)\n",
    "    \n",
    "    for _, rule in top_products.iterrows():\n",
    "        cooccur_matrix.loc[rule['antecedent'], rule['consequent']] = rule['lift']\n",
    "    \n",
    "    sns.heatmap(cooccur_matrix, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "                ax=ax4, cbar_kws={'label': 'Lift'})\n",
    "    ax4.set_title('Matriz de Asociaci√≥n (Lift)', fontweight='bold', pad=20)\n",
    "    ax4.set_xlabel('Producto Consecuente', fontweight='bold')\n",
    "    ax4.set_ylabel('Producto Antecedente', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_path}market_basket_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Gr√°fico guardado en {output_path}market_basket_analysis.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Generar visualizaciones\n",
    "plot_market_basket_analysis(rules_df, transacciones, categorias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìä RESUMEN EJECUTIVO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Transacciones analizadas: {len(transacciones):,}\")\n",
    "print(f\"Reglas descubiertas: {len(rules_df)}\")\n",
    "print(f\"Mejor regla por Lift: {rules_df.iloc[0]['antecedent']} ‚Üí {rules_df.iloc[0]['consequent']} ({rules_df.iloc[0]['lift']:.2f})\")\n",
    "print(f\"Mejor regla por Confianza: {rules_df.sort_values('confidence', ascending=False).iloc[0]['antecedent']} ‚Üí {rules_df.sort_values('confidence', ascending=False).iloc[0]['consequent']} ({rules_df.sort_values('confidence', ascending=False).iloc[0]['confidence']:.1%})\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
